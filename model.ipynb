{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3DlKbpvqNZQ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gg63zjvFX0HQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "# We will use to this to evaluate the ROUGE score of the model\n",
    "!pip install evaluate &> /dev/null\n",
    "!pip install rouge-score &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a7e679fbbd38407abe63e695b68628fb",
      "d4816d752c1144639638c86826835214",
      "a0ba9ceecca54216ade5811ec8a7f433",
      "dd4f56ceb8f14d8fb3edabbdea7ebdae",
      "5f31b43d9b6143db9af3a284310479c6",
      "4af3e59648f34b1b99e26e43e803bb8c",
      "a11137893e024abdbd859c3bdf0893b8",
      "1e5958a691a4434ab9aa86d76e8ab1c6",
      "1c58e805cbb544fbaefc700825d2fd8f",
      "4334f4f11815471fa3b26d05c6602b73",
      "75b2c9d2fcc94ec6b1882941009a5863"
     ]
    },
    "id": "bX0mYA1WnDQS",
    "outputId": "4c876319-24ec-4be9-ff2d-a2bd57534ea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e679fbbd38407abe63e695b68628fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import evaluate\n",
    "# rouge = evaluate.load('rouge')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__GnYCCWqRea"
   },
   "source": [
    "# Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUyhzrnln7TJ",
    "outputId": "237bfd94-fe57-4233-ec28-6b5d7c7ddec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-31 13:12:07--  https://www.manythings.org/anki/deu-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9780074 (9.3M) [application/zip]\n",
      "Saving to: ‘deu-eng.zip.1’\n",
      "\n",
      "deu-eng.zip.1       100%[===================>]   9.33M  4.72MB/s    in 2.0s    \n",
      "\n",
      "2023-03-31 13:12:10 (4.72 MB/s) - ‘deu-eng.zip.1’ saved [9780074/9780074]\n",
      "\n",
      "Archive:  deu-eng.zip\n",
      "replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
      "  inflating: deu.txt                 \n",
      "  inflating: _about.txt              \n",
      "Dataset size: 260434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\\n',\n",
       " 'Hi.\\tHallo!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)\\n',\n",
       " 'Hi.\\tGrüß Gott!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)\\n',\n",
       " 'Run!\\tLauf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)\\n',\n",
       " 'Run.\\tLauf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset (English to German)\n",
    "!wget https://www.manythings.org/anki/deu-eng.zip\n",
    "!unzip deu-eng.zip\n",
    "\n",
    "with open(\"deu.txt\") as f:\n",
    "  sentences = f.readlines()\n",
    "\n",
    "print(f\"Dataset size: {len(sentences)}\")\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIpw8jy9n-ms",
    "outputId": "28db2e40-a444-4a78-f614-af278d9b5859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000/50000\n",
      "05000/50000\n",
      "10000/50000\n",
      "15000/50000\n",
      "20000/50000\n",
      "25000/50000\n",
      "30000/50000\n",
      "35000/50000\n",
      "40000/50000\n",
      "45000/50000\n",
      "\n",
      "[5984, 5711, 111, 7654, 4377, 2755, 2755, 2755, 2755, 2755, 2755, 2755]\n",
      "['<sos>', 'tom', 'avoided', 'mary', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[10697, 10197, 14692, 7549, 10888, 7771, 4769, 4769, 4769, 4769, 4769, 4769, 4769, 4769, 4769]\n",
      "['<sos>', 'tom', 'wich', 'maria', 'aus', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# TODO: Replace this with our data\n",
    "###############\n",
    "\n",
    "NUM_INSTANCES = 10000 # Dataset size limit\n",
    "MAX_SENT_LEN = 12 # Maximum input sequence length\n",
    "MAX_OUTPUT_LEN = 15 # Maximum output sequence length\n",
    "\n",
    "# Used as English and German datasets\n",
    "eng_sentences, deu_sentences = [], []\n",
    "\n",
    "# Vocabluary for input encoder (English) and output decoder (German) \n",
    "eng_words, deu_words = set(), set()\n",
    "\n",
    "for i in range(NUM_INSTANCES):\n",
    "  \n",
    "  # Pick a random sample\n",
    "  rand_idx = np.random.randint(len(sentences))\n",
    "\n",
    "  # Tokenize the data (this is very bad and we probably want to use an actual tokenizer like spacy)\n",
    "  eng_sent, deu_sent = [\"<sos>\"], [\"<sos>\"]\n",
    "  eng_sent += re.findall(r\"\\w+\", sentences[rand_idx].split(\"\\t\")[0]) \n",
    "  deu_sent += re.findall(r\"\\w+\", sentences[rand_idx].split(\"\\t\")[1])\n",
    "\n",
    "  # change to lowercase\n",
    "  eng_sent = [x.lower() for x in eng_sent]\n",
    "  deu_sent = [x.lower() for x in deu_sent]\n",
    "\n",
    "  eng_sent.append(\"<eos>\")\n",
    "  deu_sent.append(\"<eos>\")\n",
    "\n",
    "  if len(eng_sent) >= MAX_SENT_LEN:\n",
    "    eng_sent = eng_sent[:MAX_SENT_LEN]\n",
    "  else:\n",
    "    for _ in range(MAX_SENT_LEN - len(eng_sent)):\n",
    "      eng_sent.append(\"<pad>\")\n",
    "\n",
    "  if len(deu_sent) >= MAX_OUTPUT_LEN:\n",
    "    deu_sent = deu_sent[:MAX_OUTPUT_LEN]\n",
    "  else:\n",
    "    for _ in range(MAX_OUTPUT_LEN - len(deu_sent)):\n",
    "      deu_sent.append(\"<pad>\")\n",
    "\n",
    "  # Add parsed sentences\n",
    "  eng_sentences.append(eng_sent)\n",
    "  deu_sentences.append(deu_sent)\n",
    "\n",
    "  # Update vocabluary\n",
    "  eng_words.update(eng_sent)\n",
    "  deu_words.update(deu_sent)\n",
    "\n",
    "# Convert vocabluaries into indexable-lists\n",
    "eng_words, deu_words = list(eng_words), list(deu_words)\n",
    "\n",
    "# Convert tokens to indicies \n",
    "for i in range(len(eng_sentences)):\n",
    "  eng_sentences[i] = [eng_words.index(x) for x in eng_sentences[i]]\n",
    "  deu_sentences[i] = [deu_words.index(x) for x in deu_sentences[i]]\n",
    "  if i % 5000 == 0:\n",
    "    print(f\"{i:05}/{len(eng_sentences)} done\")\n",
    "\n",
    "idx = 10\n",
    "print(eng_sentences[idx])\n",
    "print([eng_words[x] for x in eng_sentences[idx]])\n",
    "print(deu_sentences[idx])\n",
    "print([deu_words[x] for x in deu_sentences[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mjX9DJEpSRh"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    self.source = np.array(eng_sentences, dtype = int)\n",
    "    self.target = np.array(deu_sentences, dtype = int)\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    # get item by index\n",
    "    return self.source[idx], self.target[idx]\n",
    "  \n",
    "  def __len__(self):\n",
    "    # returns length of data\n",
    "    return len(self.source)\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "# We we sample from this dataset by sampling from the indices\n",
    "indices = list(range(len(dataset)))\n",
    "test_idx = np.random.choice(indices, size = int(len(dataset) * 0.1), replace = False)\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "val_idx = np.random.choice(train_idx, size = int(len(dataset) * 0.1), replace = False)\n",
    "train_idx = list(set(train_idx) - set(val_idx))\n",
    "train_sampler, val_sampler, test_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(val_idx), SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 128, sampler = train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size = 128, sampler = val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size = 128, sampler = test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvQRnvn8SnpE",
    "outputId": "50eb22a8-b550-43a5-e120-efd042481e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 40000\n",
      "Val dataset size: 5000\n",
      "Test dataset size: 5000\n",
      "English vocab size: 9116\n",
      "German vocab size: 16301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[5984, 5711, 2576,  ..., 4377, 2755, 2755],\n",
       "         [5984, 5711, 4044,  ..., 4377, 2755, 2755],\n",
       "         [5984, 5711, 8592,  ..., 2755, 2755, 2755],\n",
       "         ...,\n",
       "         [5984, 3027, 3969,  ..., 2755, 2755, 2755],\n",
       "         [5984, 5711,  999,  ..., 2755, 2755, 2755],\n",
       "         [5984, 4119, 6357,  ..., 2755, 2755, 2755]]),\n",
       " tensor([[10697, 10197, 11858,  ...,  4769,  4769,  4769],\n",
       "         [10697, 10197,  9397,  ...,  4769,  4769,  4769],\n",
       "         [10697, 10197,  5784,  ...,  4769,  4769,  4769],\n",
       "         ...,\n",
       "         [10697, 11598, 10604,  ...,  4769,  4769,  4769],\n",
       "         [10697,  2488,   461,  ...,  4769,  4769,  4769],\n",
       "         [10697, 14881, 12236,  ...,  4769,  4769,  4769]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_idx)}\")\n",
    "print(f\"Val dataset size: {len(val_idx)}\")\n",
    "print(f\"Test dataset size: {len(test_idx)}\")\n",
    "print(f\"English vocab size: {len(eng_words)}\")\n",
    "print(f\"German vocab size: {len(deu_words)}\")\n",
    "x, y = next(iter(train_loader)) # Get a batch\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDC-5l_VqUf9"
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DexGCI7Dpe1F"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic Attention-is-all-you-need positional encoding.\n",
    "    Source: PyTorch docs: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Source: PyTorch docs: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model with both encoder and decoder (seq2seq).\n",
    "    Prediction-time inference is done greedily (argmax of logits).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, nhead, num_layers, hidden_dim, embedding_dim, max_output_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = embedding_dim\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "        # Encoder setup\n",
    "        self.enc_embedding = nn.Embedding(enc_vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(d_model=self.dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Decoder setup\n",
    "        self.dec_embedding = nn.Embedding(dec_vocab_size, embedding_dim)\n",
    "        self.y_mask = generate_square_subsequent_mask(self.max_output_length)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.dim, dec_vocab_size)\n",
    "\n",
    "        # Init embedding and fc weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.enc_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dec_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "      \n",
    "    def forward(self, x, y) :\n",
    "        \"\"\"\n",
    "        Compute forward pass through transformer.\n",
    "        \"\"\"\n",
    "        encoded_x = self.encode(x)\n",
    "        output = self.decode(y, encoded_x)\n",
    "        return output.permute(1, 2, 0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Passes x through the transformer encoder.\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0)\n",
    "        x = self.enc_embedding(x) * math.sqrt(self.dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, y, memory):\n",
    "        \"\"\"\n",
    "        Given memory (encoder output), computes decoder output.\n",
    "        \"\"\"\n",
    "        y = y.permute(1, 0)\n",
    "        y = self.dec_embedding(y) * math.sqrt(self.dim)\n",
    "        y = self.pos_encoder(y)\n",
    "        y_mask = self.y_mask[:y.shape[0], :y.shape[0]].type_as(memory) # slice the mask\n",
    "        output = self.transformer_decoder(y, memory, y_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Method to use at inference/test time. Predict y from x one token at a time. This method is greedy\n",
    "        decoding (argmax of logits).\n",
    "        \"\"\"\n",
    "        memory = self.encode(x)\n",
    "        output_tokens = (torch.ones((x.shape[0], self.max_output_length))).type_as(x).long()\n",
    "        output_tokens[:, 0] = deu_words.index(\"<sos>\") # Initialize with SOS index\n",
    "        for o in range(1, self.max_output_length):\n",
    "            y = output_tokens[:, :o] \n",
    "            output = self.decode(y, memory)\n",
    "            output = torch.argmax(output, dim=-1)\n",
    "            output_tokens[:, o] = output[-1:]  # Get the last output token\n",
    "        return output_tokens\n",
    "\n",
    "# Model Hyperparameter Settings\n",
    "ENG_VOCAB_SIZE = len(eng_words) # English vocab size - needed for encoder embedding\n",
    "DEU_VOCAB_SIZE = len(deu_words) # German vocab size - needed for decoder embedding\n",
    "EMBEDDING_DIM = 512 # Dimension for embedding for both encoder and decoder embeddings\n",
    "HIDDEN_DIM = 1024 # Number of units in feedforward network model for both encoder and decoder layers\n",
    "NUM_HEADS = 8 # Number of multihead attention models\n",
    "NUM_LAYERS = 4 # Number of sub-encoder/sub-decoder layers in encoder/decoder\n",
    "DROPOUT = 0.3 # Dropout for positional encoding\n",
    "\n",
    "# Create model\n",
    "model = Transformer(ENG_VOCAB_SIZE, DEU_VOCAB_SIZE, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, EMBEDDING_DIM, MAX_OUTPUT_LEN)\n",
    "model.to(DEVICE)\n",
    "pass # pass to avoid outputting model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd9YlAtjS9r5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0tSStQBqWzT"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiHEczuDqQdg",
    "outputId": "574d6edc-18ab-4b88-8caa-a547e8f4be7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 6.98611 | Val Loss: 1.00325\n",
      "Epoch 005 | Train Loss: 2.44687 | Val Loss: 0.56119\n",
      "Epoch 010 | Train Loss: 1.19124 | Val Loss: 0.53306\n",
      "Epoch 015 | Train Loss: 0.57891 | Val Loss: 0.51598\n",
      "Epoch 020 | Train Loss: 0.30850 | Val Loss: 0.45541\n",
      "Epoch 025 | Train Loss: 0.19848 | Val Loss: 0.51817\n",
      "Epoch 030 | Train Loss: 0.14967 | Val Loss: 0.46885\n",
      "Epoch 035 | Train Loss: 0.12326 | Val Loss: 0.49360\n",
      "Epoch 040 | Train Loss: 0.10959 | Val Loss: 0.59720\n",
      "Epoch 045 | Train Loss: 0.09647 | Val Loss: 0.61238\n",
      "Epoch 050 | Train Loss: 0.08921 | Val Loss: 0.63954\n",
      "Epoch 055 | Train Loss: 0.08100 | Val Loss: 0.63095\n",
      "Epoch 060 | Train Loss: 0.07854 | Val Loss: 0.57894\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_loss_trace = []\n",
    "val_loss_trace = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "  train_loss = 0\n",
    "  train_total = 0\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    x, y  = x.to(DEVICE), y.to(DEVICE)\n",
    "    logits = model(x, y[:, :-1])\n",
    "    loss = criterion(logits, y[:, 1:])\n",
    "    train_loss += loss.item()\n",
    "    train_total += x.shape[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5, norm_type=2.0) # Gradient clipping\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss_trace.append(train_loss / train_total)\n",
    "\n",
    "  val_loss = 0\n",
    "  val_total = 0\n",
    "  for j, (val_x, val_y) in enumerate(val_loader):\n",
    "    val_x, val_y  = val_x.to(DEVICE), val_y.to(DEVICE)\n",
    "    val_loss += criterion(model(val_x, val_y[:, :-1]), val_y[:, 1:]).item()\n",
    "    val_total += val_x.shape[0]\n",
    "  val_loss_trace.append(val_loss / val_total)\n",
    "\n",
    "  if epoch % 5 == 0:\n",
    "    print(f\"Epoch {epoch:03} | Train Loss: {train_loss:02.5f} | Val Loss: {val_loss:02.5f}\")\n",
    "\n",
    "  torch.save(model.state_dict(), f\"./model-e{epoch}.pth\")\n",
    "\n",
    "# loss curve\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_loss_trace, 'r-')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_loss_trace, 'orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg. Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7393HQIqYcr"
   },
   "source": [
    "# Testing (ROUGE-L score)\n",
    "\n",
    "Computer the average ROUGE-L score (higher is better). Typically scores of 50 is considered good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLD1Ayr9K3gD",
    "outputId": "2a65c7b0-9d32-4065-9c8b-b1a4dc1c7466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset Average ROUGE-L score: 17.288985546095212\n"
     ]
    }
   ],
   "source": [
    "def get_sentence(tokens, vocab):\n",
    "  \"\"\"\n",
    "  Given token indicies and vocab, convert tokens back to words in vocab.\n",
    "  \"\"\"\n",
    "  return [vocab[x] for x in tokens]\n",
    "\n",
    "# def get_avg_rouge_score(model, test_loader):\n",
    "#   \"\"\"\n",
    "#   Compute the average ROUGE-L metric given the test dataset.\n",
    "#   In general, ROUGE-L compares how \"similar\" the prediction is compared to the expected output\n",
    "#   \"\"\"\n",
    "#   model.eval()\n",
    "#   avg_rouge_score = 0\n",
    "#   for i, (x, y) in enumerate(test_loader):\n",
    "\n",
    "#     x = x.to(DEVICE)\n",
    "#     y = y.to(DEVICE)\n",
    "\n",
    "#     logits = model(x, y[:, :-1])\n",
    "#     pred = model.predict(x)\n",
    "\n",
    "#     test_sentences = []\n",
    "#     pred_sentences = []\n",
    "\n",
    "#     for j in range(y.shape[0]):\n",
    "#       test_sentences.append([\" \".join(get_sentence(y[j, :], deu_words))])\n",
    "#       pred_sentences.append(\" \".join(get_sentence(pred[j, :], deu_words)))\n",
    "\n",
    "#     results = rouge.compute(predictions=pred_sentences, references=test_sentences)\n",
    "#     avg_rouge_score += results['rougeL'] \n",
    "  \n",
    "#   return avg_rouge_score\n",
    "\n",
    "# print(f\"Test dataset Average ROUGE-L score: {get_avg_rouge_score(model, test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a_5MSfoqfpn"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0zA3g7Rq36D",
    "outputId": "05e5f410-55fe-4bdc-abf4-54584c0e3f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <sos> tom concealed the fact that he had been in prison <eos>\n",
      "Expected Output: <sos> tom verschwieg die tatsache dass er im gefängnis war <eos> <pad> <pad> <pad> <pad>\n",
      "Actual Output: <sos> tom mondes heute abend die ganze woche in der schule <eos> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "src, tgt = next(iter(test_loader))\n",
    "src = src.to(DEVICE)\n",
    "tgt = tgt.to(DEVICE)\n",
    "print(f\"Input: {' '.join(get_sentence(src[0], eng_words))}\")\n",
    "print(f\"Expected Output: {' '.join(get_sentence(tgt[0], deu_words))}\")\n",
    "\n",
    "model.eval()\n",
    "outputs = model.predict(src[0,].unsqueeze(0))\n",
    "print(f\"Actual Output: {' '.join(get_sentence(outputs.squeeze(0), deu_words))}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c58e805cbb544fbaefc700825d2fd8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1e5958a691a4434ab9aa86d76e8ab1c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4334f4f11815471fa3b26d05c6602b73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4af3e59648f34b1b99e26e43e803bb8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f31b43d9b6143db9af3a284310479c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75b2c9d2fcc94ec6b1882941009a5863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ba9ceecca54216ade5811ec8a7f433": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5958a691a4434ab9aa86d76e8ab1c6",
      "max": 6270,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c58e805cbb544fbaefc700825d2fd8f",
      "value": 6270
     }
    },
    "a11137893e024abdbd859c3bdf0893b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7e679fbbd38407abe63e695b68628fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4816d752c1144639638c86826835214",
       "IPY_MODEL_a0ba9ceecca54216ade5811ec8a7f433",
       "IPY_MODEL_dd4f56ceb8f14d8fb3edabbdea7ebdae"
      ],
      "layout": "IPY_MODEL_5f31b43d9b6143db9af3a284310479c6"
     }
    },
    "d4816d752c1144639638c86826835214": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4af3e59648f34b1b99e26e43e803bb8c",
      "placeholder": "​",
      "style": "IPY_MODEL_a11137893e024abdbd859c3bdf0893b8",
      "value": "Downloading builder script: 100%"
     }
    },
    "dd4f56ceb8f14d8fb3edabbdea7ebdae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4334f4f11815471fa3b26d05c6602b73",
      "placeholder": "​",
      "style": "IPY_MODEL_75b2c9d2fcc94ec6b1882941009a5863",
      "value": " 6.27k/6.27k [00:00&lt;00:00, 138kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
